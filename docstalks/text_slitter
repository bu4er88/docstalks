from transformers import GPT2TokenizerFast
from langchain_text_splitters import CharacterTextSplitter


tokenizer = GPT2TokenizerFast.from_pretrained()

with open("../../../state_of_the_union.txt") as f:
    state_of_the_union = f.read()

text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(
    tokenizer, chunk_size=100, chunk_overlap=0
)
texts = text_splitter.split_text(state_of_the_union)